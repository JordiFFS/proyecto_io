"""
Archivo: gemini_pruebas.py
Integrado en resolucion_ruta_mas_corta.py
Genera un an√°lisis acad√©mico usando Gemini (Google)
"""

import google.generativeai as genai
import os

# ======================================================
# üîë CONFIGURACI√ìN DEL TOKEN DE GEMINI
# ======================================================
def configurar_gemini():
    """Configura Gemini con la API key"""
    api_key = "AIzaSyCgqQn0Ok2n91mHooDByTUHbgnM8M2LuEg"
    if not api_key:
        raise ValueError("GEMINI_API_KEY no est√° configurada en variables de entorno")
    genai.configure(api_key=api_key)

try:
    configurar_gemini()
except ValueError as e:
    print(f"‚ö†Ô∏è Advertencia: {e}")

MODEL_NAME = "gemini-2.0-flash"
model = genai.GenerativeModel(MODEL_NAME)


def generar_analisis_gemini(origen, rutas, iteraciones, total_nodos) -> str:
    """
    Genera an√°lisis acad√©mico usando Gemini

    Args:
        origen: Nodo origen del an√°lisis
        rutas: Lista de rutas con destino, distancia y ruta √≥ptima
        iteraciones: N√∫mero de iteraciones del algoritmo
        total_nodos: Total de nodos en la red

    Returns:
        str: An√°lisis acad√©mico generado por Gemini
    """

    rutas_texto = ""
    for r in rutas:
        rutas_texto += (
            f"- Destino: {r['destino']}\n"
            f"  Distancia m√≠nima: {r['distancia']}\n"
            f"  Ruta √≥ptima: {r['ruta']}\n\n"
        )

    prompt = f"""INSTRUCCI√ìN:
Redacta un texto acad√©mico EN ESPA√ëOL con lenguaje matem√°tico formal.
NO repitas el enunciado del problema.
RESPONDE √öNICAMENTE con las secciones solicitadas.

ESTRUCTURA OBLIGATORIA (resp√©tala exactamente):

AN√ÅLISIS DE SENSIBILIDAD:
(explica c√≥mo cambios en los pesos afectan las rutas √≥ptimas)

CONCLUSIONES:
(interpreta el comportamiento matem√°tico del sistema)

RECOMENDACIONES:
(prop√≥n mejoras a la red y decisiones operativas)

DATOS DEL PROBLEMA:
Nodo origen: {origen}
Total de nodos: {total_nodos}
Iteraciones: {iteraciones}

RUTAS √ìPTIMAS CALCULADAS:
{rutas_texto}"""

    try:
        response = model.generate_content(
            prompt,
            generation_config={
                "temperature": 0.3,
                "max_output_tokens": 1000,
                "top_p": 0.95,
            }
        )

        respuesta = response.text.strip()
        return respuesta

    except Exception as e:
        return f"Error al generar an√°lisis con Gemini: {str(e)}"