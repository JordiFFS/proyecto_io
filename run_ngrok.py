"""
ia/analista.py
M√≥dulo simplificado para an√°lisis con OpenAI API oficial
(Sin necesidad de ngrok)
"""

import os
import json
import streamlit as st
from typing import Dict, List, Optional
from dotenv import load_dotenv

# Cargar variables de entorno
load_dotenv()

try:
    from openai import OpenAI, APIError, APIConnectionError, RateLimitError
    OPENAI_DISPONIBLE = True
except ImportError:
    OPENAI_DISPONIBLE = False


class AnalistaIA:
    """
    Clase para an√°lisis autom√°tico usando OpenAI API oficial
    Sin necesidad de ngrok ni servidores locales
    """

    def __init__(self):
        """Inicializa el cliente de OpenAI"""
        self.api_key = os.getenv("OPENAI_API_KEY")
        self.model = os.getenv("OPENAI_MODEL", "gpt-4o")

        # Log de diagn√≥stico
        self.disponible = False
        self.cliente = None
        self.error_msg = ""

        if not self.api_key:
            self.error_msg = "OPENAI_API_KEY no configurada en .env"
            return

        if not OPENAI_DISPONIBLE:
            self.error_msg = "OpenAI no instalada. Ejecuta: pip install openai"
            return

        try:
            # Inicializar cliente con OpenAI oficial
            self.cliente = OpenAI(api_key=self.api_key)
            self.disponible = True
            st.session_state.ia_status = "‚úÖ Conectado a OpenAI"
        except Exception as e:
            self.error_msg = f"Error al conectar: {str(e)}"

    def analizar_ejercicio(
        self,
        tipo_problema: str,
        datos_entrada: Dict,
        resultado: Dict,
        metadata: Dict = None
    ) -> str:
        """
        Analiza un ejercicio de optimizaci√≥n
        """
        if not self.disponible or not self.cliente:
            return self._analisis_fallback_ejercicio(tipo_problema, resultado)

        prompt = f"""
Eres un experto en Investigaci√≥n de Operaciones e Ingenier√≠a Industrial.

Analiza el siguiente ejercicio de optimizaci√≥n:

**Tipo de Problema:** {tipo_problema}
**Datos de Entrada:** {json.dumps(datos_entrada, indent=2, default=str)}
**Resultado Obtenido:** {json.dumps(resultado, indent=2, default=str)}

Por favor proporciona:
1. **Interpretaci√≥n del resultado** - Qu√© significa el valor √≥ptimo obtenido
2. **Validaci√≥n de la soluci√≥n** - ¬øEs viable y tiene sentido?
3. **Conclusiones clave** - Hallazgos m√°s importantes
4. **Recomendaciones pr√°cticas** - C√≥mo usar estos resultados

S√© conciso pero informativo (m√°ximo 400 palabras).
Usa formato markdown con vi√±etas donde sea apropiado.
"""

        try:
            response = self.cliente.chat.completions.create(
                model=self.model,
                messages=[
                    {
                        "role": "user",
                        "content": prompt
                    }
                ],
                temperature=0.7,
                max_tokens=800,
                timeout=30
            )
            return response.choices[0].message.content
        except APIConnectionError as e:
            return f"‚ùå Error de conexi√≥n: {str(e)}. Verifica tu conexi√≥n a internet."
        except RateLimitError:
            return "‚è≥ L√≠mite de rate alcanzado. Intenta en unos segundos."
        except APIError as e:
            return f"‚ùå Error de OpenAI: {str(e)}"
        except Exception as e:
            return self._analisis_fallback_ejercicio(tipo_problema, resultado)

    def analizar_sensibilidad(
        self,
        tipo_problema: str,
        parametros_sensibles: Dict,
        resultado_actual: float,
        restricciones: Dict = None
    ) -> str:
        """
        Realiza an√°lisis de sensibilidad autom√°tico
        """
        if not self.disponible or not self.cliente:
            return self._analisis_fallback_sensibilidad(parametros_sensibles)

        prompt = f"""
Realiza un an√°lisis de sensibilidad detallado para una soluci√≥n de optimizaci√≥n:

**Tipo de Problema:** {tipo_problema}
**Valor √ìptimo Actual:** {resultado_actual}
**Par√°metros Sensibles:** {json.dumps(parametros_sensibles, indent=2, default=str)}
{f'**Restricciones:** {json.dumps(restricciones, indent=2, default=str)}' if restricciones else ''}

Por favor proporciona:
1. **Par√°metros Cr√≠ticos** - Cu√°les tienen mayor impacto en la soluci√≥n
2. **Rangos de Variabilidad** - Dentro de qu√© l√≠mites puede variar cada par√°metro
3. **Puntos de Quiebre** - Valores cr√≠ticos donde cambia la soluci√≥n
4. **Estrategia de Robustez** - C√≥mo hacer la soluci√≥n m√°s resiliente

S√© espec√≠fico con n√∫meros y porcentajes (m√°ximo 350 palabras).
"""

        try:
            response = self.cliente.chat.completions.create(
                model=self.model,
                messages=[
                    {
                        "role": "user",
                        "content": prompt
                    }
                ],
                temperature=0.7,
                max_tokens=700,
                timeout=30
            )
            return response.choices[0].message.content
        except Exception as e:
            return self._analisis_fallback_sensibilidad(parametros_sensibles)

    def generar_resumen_ejecutivo(
        self,
        tipo_problema: str,
        objetivo: str,
        metricas: Dict,
        recomendaciones: List[str] = None,
        contexto_empresa: str = "Coca-Cola"
    ) -> str:
        """
        Genera un resumen ejecutivo profesional
        """
        if not self.disponible or not self.cliente:
            return self._resumen_fallback(metricas, contexto_empresa)

        prompt = f"""
Genera un resumen ejecutivo profesional para la gerencia de {contexto_empresa}:

**Problema Resuelto:** {tipo_problema}
**Objetivo:** {objetivo}

**M√©tricas Clave:**
{json.dumps(metricas, indent=2, default=str)}

{f'**Recomendaciones:** {", ".join(recomendaciones)}' if recomendaciones else ''}

**Formato solicitado:**
1. **Situaci√≥n** (1-2 l√≠neas) - Contexto del problema
2. **Soluci√≥n** (3-4 vi√±etas) - Resultados principales y valor generado
3. **Impacto** (1-2 l√≠neas) - Beneficio cuantificable para la empresa
4. **Pr√≥ximos Pasos** (3-4 vi√±etas) - Acciones recomendadas

**Estilo:** Ejecutivo, directo, enfocado en valor empresarial. M√°ximo 250 palabras.
Usa formato markdown. Incluye m√©tricas cuantitativas cuando sea posible.
"""

        try:
            response = self.cliente.chat.completions.create(
                model=self.model,
                messages=[
                    {
                        "role": "user",
                        "content": prompt
                    }
                ],
                temperature=0.7,
                max_tokens=600,
                timeout=30
            )
            return response.choices[0].message.content
        except Exception as e:
            return self._resumen_fallback(metricas, contexto_empresa)

    def generar_interpretacion(
        self,
        tipo_problema: str,
        resultado: float,
        detalles_problema: Dict
    ) -> str:
        """
        Genera una interpretaci√≥n amigable del resultado
        """
        if not self.disponible or not self.cliente:
            return f"**Resultado √≥ptimo:** {resultado}"

        prompt = f"""
Proporciona una interpretaci√≥n amigable de los siguientes resultados de optimizaci√≥n:

**Tipo de Problema:** {tipo_problema}
**Resultado √ìptimo:** {resultado}
**Detalles:** {json.dumps(detalles_problema, indent=2, default=str)}

Explica qu√© significa este resultado en t√©rminos simples y pr√°cticos.
¬øQu√© acci√≥n debe tomar el usuario con este resultado?
M√°ximo 200 palabras. Tono profesional pero accesible.
"""

        try:
            response = self.cliente.chat.completions.create(
                model=self.model,
                messages=[
                    {
                        "role": "user",
                        "content": prompt
                    }
                ],
                temperature=0.7,
                max_tokens=400,
                timeout=30
            )
            return response.choices[0].message.content
        except Exception as e:
            return f"**Resultado √≥ptimo obtenido:** {resultado}"

    # M√©todos fallback
    def _analisis_fallback_ejercicio(self, tipo: str, resultado: Dict) -> str:
        """An√°lisis fallback sin IA"""
        costo = resultado.get('costo_total', resultado.get('valor_optimo', resultado.get('distancia_total', 0)))
        return f"""
### üìä An√°lisis del Ejercicio (Modo B√°sico)

**Tipo de Problema:** {tipo}
**Resultado √ìptimo:** {costo:.2f}

#### Validaci√≥n
‚úì La soluci√≥n ha sido calculada correctamente usando el algoritmo especificado.

#### Recomendaciones
- Verifica que la soluci√≥n satisface todas las restricciones
- Compara con soluciones alternativas si es posible
- Documenta los hallazgos principales

‚ö†Ô∏è **Nota:** Para an√°lisis con IA, configura tu API key de OpenAI en `.env`
"""

    def _analisis_fallback_sensibilidad(self, parametros: Dict) -> str:
        """An√°lisis de sensibilidad fallback"""
        return """
### üîç An√°lisis de Sensibilidad (Modo B√°sico)

#### Par√°metros Identificados
Se han identificado par√°metros clave en el modelo de optimizaci√≥n.

#### Recomendaciones
- Enfoca atenci√≥n en los par√°metros con mayor variabilidad
- Establece l√≠mites de tolerancia para cada par√°metro cr√≠tico
- Monitorea cambios en estos par√°metros durante la ejecuci√≥n

‚ö†Ô∏è **Nota:** Para an√°lisis detallado con IA, configura tu API key en `.env`
"""

    def _resumen_fallback(self, metricas: Dict, empresa: str = "Coca-Cola") -> str:
        """Resumen ejecutivo fallback"""
        metricas_str = "\n".join([f"‚Ä¢ **{k}:** {v}" for k, v in list(metricas.items())[:5]])
        return f"""
### üìã Resumen Ejecutivo - {empresa}

#### Resultados Principales
{metricas_str}

#### Recomendaci√≥n
Implementar la soluci√≥n √≥ptima identificada para maximizar eficiencia operativa.

‚ö†Ô∏è **Nota:** Para resumen ejecutivo con IA, configura tu API key en `.env`
"""

    def verificar_disponibilidad(self) -> bool:
        """Verifica si la conexi√≥n a OpenAI est√° disponible"""
        return self.disponible

    def mostrar_estado_ia(self) -> None:
        """Muestra el estado de la conexi√≥n IA en Streamlit"""
        if self.disponible:
            st.success("‚úÖ IA disponible - An√°lisis autom√°tico habilitado")
        elif self.error_msg:
            st.warning(f"‚ö†Ô∏è IA no disponible: {self.error_msg}")
        else:
            st.info("‚ÑπÔ∏è An√°lisis con IA deshabilitado")

    def obtener_estado(self) -> Dict:
        """Retorna estado detallado de la conexi√≥n"""
        return {
            "disponible": self.disponible,
            "modelo": self.model,
            "error": self.error_msg,
            "api_key_presente": bool(self.api_key)
        }


# Funci√≥n auxiliar para usar en Streamlit
def obtener_analista() -> Optional[AnalistaIA]:
    """
    Obtiene una instancia del analista IA con cach√© de Streamlit
    """
    @st.cache_resource
    def _crear_analista():
        try:
            analista = AnalistaIA()
            return analista
        except Exception as e:
            return None

    return _crear_analista()


def mostrar_diagnostico_ia():
    """Muestra diagn√≥stico de la conexi√≥n IA (para debugging)"""
    analista = obtener_analista()

    if analista:
        estado = analista.obtener_estado()

        st.subheader("üîç Diagn√≥stico IA")

        col1, col2, col3 = st.columns(3)

        with col1:
            if estado["disponible"]:
                st.success("‚úÖ Conectado")
            else:
                st.error("‚ùå Desconectado")

        with col2:
            st.info(f"Modelo: {estado['modelo']}")

        with col3:
            if estado["api_key_presente"]:
                st.success("‚úÖ API Key presente")
            else:
                st.warning("‚ö†Ô∏è API Key no configurada")

        if estado["error"]:
            st.error(f"Error: {estado['error']}")